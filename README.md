# Machine-learning

--------------------
## KNN算法
给一个训练数据集和一个新的实例，在训练数据集中找出与这个新实例最近的k个训练实例，然后统计最近的k个训练实例中所属类别技术最多的那个类，就是新实例的类。
### 三要素：
- k值的选择
- 距离的度量（常见的距离度量有欧式距离，马氏距离，夹角余弦等）
- 分类决策规则（多数表决规则）

### k值的选择：
- 1.k值越小表明模型越复杂，更加容易过拟合
- 2.但是k值越大，模型越简单，如果k=N的时候就表明无论什么点都是训练集中类别最多的那个类。
所以一般k会取一个较小的值，然后用交叉验证来确定（关于[交叉验证](https://zhuanlan.zhihu.com/p/24825503?refer=rdatamining))
### KNN的回归
在找到最近的k个实例之后，可以计算着k个实例的平均值作为预测值。或者还可以给这k个实例添加一个权重再求平均值，这个权重与度量距离成反比（越近权重越大）。
### 优缺点
knn算法的优点：
- 思想简单，理论成熟，既可以用来做分类也可以用来做回归；
- 可用于非线性分类；
- 训练时间复杂度未O(n);
- 准确度高，对数据没有假设，对outlier不敏感

缺点：
- 计算量大
- 样本不平衡问题（即有些类别的样本数量很多，而其他的样本的数量很少）
- 需要大量的内存

-----------

## 朴素贝叶斯
事件A和B同时发生的概率为在A发生的情况下发生B或者在B发生的情况下发生A

P(A∩B)=P(A)∗P(B|A)=P(B)∗P(A|B)

所以有：

P(A|B)=P(B|A)∗P(A)P(B)

对于给出的待分类项，求解在此项目出现的条件下各个目标类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。

### 工作原理
- 1.假设现在有样本x=（a1,a2,a3,...,an）这个待分类项（并认为x里面的特征独立）
- 2.再假设现在有分类目标Y={y1,y2,y3,...,yn}
- 3.那么max(P(y1|x),P(y2|x),...,P(yn|x))就是最终的分类类别
- 4.而P(yi|x)=P(x|yi）* P(yi)P(x)
- 5.因为x对于每个分类目标来说都一样，所以就是求max（P(x|yi)* P(yi)）
- 6.P(x|yi)* P(yi) = p(yi)* ∏i(P(ai|yi)) 
- 7.而具体的P(ai|yi)和P(yi)都是能从训练样本中统计出来

P(ai|yi)表示该类别下该特征出现的概率

P(yi)表示全部类别中这个类别出现的概率

**一句话概要：朴素贝叶斯算法普遍用在文本处理中的垃圾邮件过滤，先计算联合概率分布，然后在利用贝叶斯公式计算给定某个样本数据后，被分到每个类别的概率分别是多少，然后取被分到概率最大的类别作为该样本数据的类别**

朴素贝叶斯算法的【朴素】在于 这样一个假设：每个单词（特征）出现的可能性完全独立，每个单词（特征）对于判定文档类型同等重要。

  
  
