# Machine-learning

--------------------
## KNN算法
给一个训练数据集和一个新的实例，在训练数据集中找出与这个新实例最近的k个训练实例，然后统计最近的k个训练实例中所属类别技术最多的那个类，就是新实例的类。
### 三要素：
- k值的选择
- 距离的度量（常见的距离度量有欧式距离，马氏距离，夹角余弦等）
- 分类决策规则（多数表决规则）

### k值的选择：
- 1.k值越小表明模型越复杂，更加容易过拟合
- 2.但是k值越大，模型越简单，如果k=N的时候就表明无论什么点都是训练集中类别最多的那个类。
所以一般k会取一个较小的值，然后用交叉验证来确定（关于[交叉验证](https://zhuanlan.zhihu.com/p/24825503?refer=rdatamining))
### KNN的回归
在找到最近的k个实例之后，可以计算着k个实例的平均值作为预测值。或者还可以给这k个实例添加一个权重再求平均值，这个权重与度量距离成反比（越近权重越大）。
### 优缺点
knn算法的优点：
- 思想简单，理论成熟，既可以用来做分类也可以用来做回归；
- 可用于非线性分类；
- 训练时间复杂度未O(n);
- 准确度高，对数据没有假设，对outlier不敏感

缺点：
- 计算量大
- 样本不平衡问题（即有些类别的样本数量很多，而其他的样本的数量很少）
- 需要大量的内存

-----------

## 朴素贝叶斯
事件A和B同时发生的概率为在A发生的情况下发生B或者在B发生的情况下发生A

P(A∩B)=P(A)∗P(B|A)=P(B)∗P(A|B)

所以有：

P(A|B)=P(B|A)∗P(A)P(B)

对于给出的待分类项，求解在此项目出现的条件下各个目标类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。

<img src="https://latex.codecogs.com/gif.latex?p(c_{i})=\frac{p(w|c_i)p(c_i)}{p(w)}" title="p(c_{i})=\frac{p(w|c_i)p(c_i)}{p(w)}" />



### 工作原理
- 1.假设现在有样本x=（a1,a2,a3,...,an）这个待分类项（并认为x里面的特征独立）
- 2.再假设现在有分类目标Y={y1,y2,y3,...,yn}
- 3.那么max(P(y1|x),P(y2|x),...,P(yn|x))就是最终的分类类别
- 4.而P(yi|x)=P(x|yi）* P(yi)P(x)
- 5.因为x对于每个分类目标来说都一样，所以就是求max（P(x|yi)* P(yi)）
- 6.P(x|yi)* P(yi) = p(yi)* ∏i(P(ai|yi)) 
- 7.而具体的P(ai|yi)和P(yi)都是能从训练样本中统计出来

P(ai|yi)表示该类别下该特征出现的概率

P(yi)表示全部类别中这个类别出现的概率

**一句话概要：朴素贝叶斯算法普遍用在文本处理中的垃圾邮件过滤，先计算联合概率分布，然后在利用贝叶斯公式计算给定某个样本数据后，被分到每个类别的概率分别是多少，然后取被分到概率最大的类别作为该样本数据的类别**

朴素贝叶斯算法的【朴素】在于 这样一个假设：每个单词（特征）出现的可能性完全独立，每个单词（特征）对于判定文档类型同等重要。

**以垃圾邮件过滤举例整个朴素贝叶斯算法的流程：**

首先是建立单词表（也就是建立w）和用单词表表示所有的训练文档。我们便利所有的训练文档，把所有出现过的单词去重放进一个数组里面。然后用这个单词表w表示我们所有的训练文档，每条训练文档被表示成和单词表数组等长的一个数组，如果是词集模型，就用0,1分别表示对应位置的单词在这条文档里是否出现过；如果是词袋模型，就用对用位置的数字表示对应位置单词出现次数，这个数组叫做这条文档的词向量。同时，还需要一个标注数组，这个数组大小和训练文档的条数相同，用于记录这些蓄念文当哪几条是垃圾邮件，是的标0，不是的标1.

我们用p(c0|w)和p(c1|w)分别代表这个词向量所对应的文档属于垃圾邮件或正常邮件的概率，如果p(c0|w) > p(c1|w)那我们就认定其为垃圾邮件。

当接到新文档后，先将其转化为词向量，然后代入贝叶斯公式计算，根据贝叶斯公式，需要求出p(c0|w)和p(c1|w)，因为分母p(w)对于该条文档来讲是个常数，与c取c0还是c1无关，所以可以不计算，只需要计算分子p(c)和p(w|c)。p(c0)、p(c1)分别代表文档是垃圾邮件和正常邮件的概率，这个很好算，用前面的标注数组就能算出，也就是用训练文档是垃圾邮件和正常邮件的概率来代替。

然后我们只需要计算这个新文档的p(w|c0)和p(w|c1)，也就是c0（垃圾邮件）和c1（正常邮件）中出现该文档的概率（即该文档所包含的每个单词出现概率的乘积，文档中几次出现该单词，就乘以几次这个单词的出现概率），即可根据p(w|c)乘p(c)的大小判定该样本的类别。

新样本的p(w|c0)和p(w|c1)如何计算？上面说了，是用两种邮件的训练样本中每个词（包含在新样本中的每个词）出现概率的乘积求出，那两种邮件的训练样本每个词出现的概率怎么求？我们把之前的训练文档按照垃圾邮件和正常邮件分开处理，用每个词的出现次数（这里用词袋模型）除以总词数，即可得出垃圾邮件和正常邮件训练样本中所有单词的出现概率，然后根据单词在新样本中的出现次数，将其出现概率相乘，得到新文档的p(w|c0)和p(w|c1)，然后分别乘以p(c0)、p(c1)，比较其大小，即可得出新样本的类别。

朴素贝叶斯分类算法的思想如上文所述，但还存在着一些特殊情况需要处理，比如我们上面说计算新文档的p(w|c0)和p(w|c1)时，要分别用到新文档中包含的单词分别在垃圾邮件样本和正常邮件样本中所出现的概率，但要是新文档中有个单词在垃圾邮件（或正常邮件）中从来都没出现过怎么办，那概率就是0，再乘以其他单词的在垃圾邮件（或正常邮件）中的出现概率，最终结果一定是0，也就是这条文档已经被宣判不可能是垃圾邮件（或正常邮件）了，这样太过武断，所以我们做一下修改，在训练时，让每个单词的初始出现次数都设置为1，总出现次数初始设置为2，以避免上面提到的问题。

那要是有些单词在新文档里面出现次数为0怎么办？那就不乘进去。

还有个小问题就是因为乘的数都小于1，乘的次数又那么多，所以会出现下溢出，最后的结果四舍五入都变成0了，所以我们采用自然对数函数来解决这个问题，乘变成了加，乘方变成了直接相乘，如果出现次数为0，那一项就为0，不加进去，也就相当于不乘进去了，这里不再详细讲。

还有一点，如果某个特征取值是连续的怎么办，就不适宜按照某个特定值计算概率了，上例中的特征都是单词的出现次数，都是离散的，但是如果我想再增加考虑比如邮件的发送时间，时间是连续的，如果不是同一秒发送的邮件就被认为不同，那对于分类几乎没有帮助（可以理解为上例中我们的单词表里又增加了许多单词，但是这些单词不论在垃圾邮件还是正常邮件里出现的概率都极低，对于分类没有帮助），解决方法是我们按时间段划分，比如分为上午、下午、傍晚、深夜，即相当于在单词表中添加了四个单词，不同的文档都会包含这四个单词中的其中一个，如果垃圾邮件的发送时间绝大多数都在深夜，那在使用垃圾邮件训练样本训练时，深夜这个单词的出现概率就会非常高，如果新文档也是在深夜发送，就会显著提高这条新文档被认为是垃圾邮件的概率。

另外还有一点就是最开始概要的时候所提到的生成方法和判别方法，生成方法和判别方法是对监督学习算法的分类，前面讲到的kNN和决策树是判别方法，而这部分讲的朴素贝叶斯分类算法是生成方法，由判别方法学到的模型叫做判别模型，由生成方法学到的模型叫做生成模型。

判别方法由数据直接学习决策函数Y=f(X)或者条件概率分布P(Y|X)作为预测的模型，即判别模型。基本思想是有限样本条件下建立判别函数，不考虑样本的产生模型，直接研究预测模型。典型的判别模型包括k近邻，感知机，决策树，支持向量机等。

生成方法由数据学习联合概率密度分布P(X,Y)，然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型：P(Y|X)= P(X,Y)/ P(X)。基本思想是首先建立样本的联合概率概率密度模型P(X,Y)，然后再得到后验概率P(Y|X)，再利用它进行分类。

判别方法更多的关注各个类别之间的区别，而生成方法更多关注各个类别内部的相似度。判别方法画一条线，明确这种差别，线左边是类别A，线右边是类别B；生成方法拿样本去比较两个类别的已有数据，看看这个样本生成自哪个类别的可能性更大。

由生成模型可以得到判别模型，但由判别模型得不到生成模型。
