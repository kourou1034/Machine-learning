# Machine-learning

## KNN算法
给一个训练数据集和一个新的实例，在训练数据集中找出与这个新实例最近的k个训练实例，然后统计最近的k个训练实例中所属类别技术最多的那个类，就是新实例的类。
### 三要素：
- k值的选择
- 距离的度量（常见的距离度量有欧式距离，马氏距离，夹角余弦等）
- 分类决策规则（多数表决规则）

### k值的选择：
- 1.k值越小表明模型越复杂，更加容易过拟合
- 2.但是k值越大，模型越简单，如果k=N的时候就表明无论什么点都是训练集中类别最多的那个类。
所以一般k会取一个较小的值，然后用交叉验证来确定（关于[交叉验证](https://zhuanlan.zhihu.com/p/24825503?refer=rdatamining))
### KNN的回归
在找到最近的k个实例之后，可以计算着k个实例的平均值作为预测值。或者还可以给这k个实例添加一个权重再求平均值，这个权重与度量距离成反比（越近权重越大）。
### 优缺点
knn算法的优点：
- 思想简单，理论成熟，既可以用来做分类也可以用来做回归；
- 可用于非线性分类；
- 训练时间复杂度未O(n);
- 准确度高，对数据没有假设，对outlier不敏感

缺点：
- 计算量大
- 样本不平衡问题（即有些类别的样本数量很多，而其他的样本的数量很少）
- 需要大量的内存

## 朴素贝叶斯
